# Control, See: Guided Text-to-Image Generation via In-context Reference Representation

## Abstract

Guided text-to-image generation deals with the task of generating images from textual descriptions while constraining the output generation using a set of references. Typically, this reference consists of a set of RGB images, depicting the desired characteristics of the generated image. Motivation for this may include sketch-to-image [1], style transfer [2], character consistency [3], etc. However, it is relatively difficult to maintain similar conformity to a reference when it is provided in the form of text. In this work, I explore guided generation of images from textual descriptions with in-context reference representation. To demonstrate the effectiveness of the approach, I present a series of images generated using DALL·E 3 [4] and showcase its ability to retain character consistency and scene genre preservation across multiple generations.

The following catalogue primarily presents a key frame storyboard in cinematic style for a short story of one particular genre -- horror. Additionally, a curated set of images generated with a different style (cartoons) and an alternate genre (Rom-Com, Indo-Western) are also showcased. Last but not the least, keeping in line with the theme of the content, a few blooper outputs are also included in this list. The album is divided into multiple sections, each containing a set of images generated from a single prompt. Prompts are designed to contain (a) thematic description: style of the image and desired genre, (b) scene description: background, foreground, etc., and (c) detailed character descriptions: facial features, attire, pose, etc.

## References

1. Voynov, A., Aberman, K., & Cohen-Or, D. (2023, July). Sketch-guided text-to-image diffusion models. In ACM SIGGRApH 2023 conference proceedings (pp. 1-11).
2. Wang, H., Xing, P., Huang, R., Ai, H., Wang, Q., & Bai, X. (2024). InstantStyle-plus: Style transfer with content-preserving in text-to-image generation. arXiv preprint arXiv:2407.00788.
3. Avrahami, O., Hertz, A., Vinker, Y., Arar, M., Fruchter, S., Fried, O., Cohen-Or, D., & Lischinski, D. (2024, July). The chosen one: Consistent characters in text-to-image diffusion models. In ACM SIGGRAPH 2024 conference papers (pp. 1-12).
4. DALL·E 3. (n.d.). OpenAI. https://openai.com/index/dall-e-3/ (accessed on 09 Mar, 2025).
